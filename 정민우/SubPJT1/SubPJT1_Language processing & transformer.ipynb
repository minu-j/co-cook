{"cells":[{"cell_type":"markdown","metadata":{"id":"GiWulKAdmAHX"},"source":["# Language processing & Transformer"]},{"cell_type":"markdown","metadata":{"id":"9KsBGZpKkWki"},"source":["음성 AI를 위한 자연어 처리와 Transformer의 핵심 구조인 Multi-head Attention을 구현하는 실습입니다.\n","1. 텍스트 전처리 과정 이해\n","    - tokenizing\n","    - cleaning\n","2. Multi-head attention 및 self-attention 구현.\n","3. 각 과정에서 일어나는 연산과 input/output 형태 이해."]},{"cell_type":"markdown","metadata":{"id":"8qRU5DFY2OM8"},"source":["### 필요 패키지 install & import"]},{"cell_type":"code","execution_count":27,"metadata":{"vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"id":"MIpHB89Qcy3-","executionInfo":{"status":"ok","timestamp":1677729746714,"user_tz":-540,"elapsed":7818,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"ac2150bc-b29b-4a1b-aafd-ab47eee32b62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n"]}],"source":["!pip install konlpy"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"lDtMioSQQ1bB","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677729746714,"user_tz":-540,"elapsed":11,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","import re\n","import torch\n","import math\n","\n","from konlpy.tag import Okt\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"HH0VdC4uJJVG"},"source":["## Req. 1-1 텍스트 전처리"]},{"cell_type":"markdown","metadata":{"id":"QBiZObgRep_Q"},"source":["주어진 문장 5개를 cleaning, tokenizing 한 뒤 정수 인코딩 하시오.  \n","\n","원하는 다른 tokenizer를 사용해도 좋습니다."]},{"cell_type":"code","execution_count":41,"metadata":{"vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"id":"RGBbh5lbcy3_","executionInfo":{"status":"ok","timestamp":1677731220832,"user_tz":-540,"elapsed":340,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"ca78a003-4998-48a0-e011-2c3950c4b2f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[5, 6, 7, 2, 8, 9, 3, 10, 11], [1, 12, 13, 14, 15, 16, 4], [17, 1, 18, 19, 20, 21, 22, 23, 4], [24, 25, 26, 27, 1, 28, 29, 30, 31, 32], [33, 34, 2, 35, 36, 37, 38, 3]]\n"]}],"source":["sentences = [[\"안녕하세요 음성 AI 실!@습에 오신 것을 환영#$^&@$&$합니다.\"], [\"이네들은 7895435너무나 멀리 있습니다.\"], \n","[\"계절이 지나가는 하늘에는가을로 가&^%@!$!^득 차 있습니다.\"], [\"아직 나의 청!@$!%춘이 다하지!@% 않은 까닭입니다.\"], [\"가슴 속에 하!@$나 둘 새겨지는 별을\"]]\n","\n","okt = Okt()\n","tokenizer = Tokenizer()\n","\n","cleaned_sentences = []\n","for sentence in sentences:\n","    # 문장에서 특수문자 제거\n","    cleaned = re.sub(r\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", sentence[0])\n","    cleaned_sentences.append(cleaned)\n","\n","# Konlpy의 Okt를 사용해 문장을 토큰화\n","tokenized_sentences = []\n","for sentence in cleaned_sentences:\n","    tokens = okt.morphs(sentence)\n","    tokenized_sentences.append(tokens)\n","\n","# Tokenizer를 사용해 정수 인코딩\n","tokenizer.fit_on_texts(tokenized_sentences)\n","encoded_sentences = tokenizer.texts_to_sequences(tokenized_sentences)\n","\n","print(encoded_sentences)"]},{"cell_type":"markdown","metadata":{"id":"b3ZB7u4Bcy3_"},"source":["결과는 다음과 같이 나와야 합니다.  \n","\n","\n","[[5, 6, 7, 2, 8, 9, 3, 10, 11],  \n"," [1, 12, 13, 14, 15, 16, 4],  \n"," [17, 1, 18, 19, 20, 21, 22, 23, 4],  \n"," [24, 25, 26, 27, 1, 28, 29, 30, 31, 32],  \n"," [33, 34, 2, 35, 36, 37, 38, 3]]  \n"," "]},{"cell_type":"markdown","metadata":{"id":"vvPAbLTrcy4A"},"source":["## Req. 1-2 Multi-head self-attention 구조 익히기"]},{"cell_type":"markdown","metadata":{"id":"2KJqRm6ucy4A"},"source":["위에서 전처리한 데이터를 가져와 아래 과정을 실행하면서 시퀀스 입력이 multi-head self attention으로 어떻게 모델링 되는지 파악하시오."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"e9ULZIqTenSc","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731229522,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["pad_id = 0\n","vocab_size = 40\n","\n","data = [[5, 6, 7, 2, 8, 9, 3, 10, 11], [1, 12, 13, 14, 15, 16, 4], [17, 1, 18, 19, 20, 21, 22, 23, 4], [24, 25, 26, 27, 1, 28, 29, 30, 31, 32], [33, 34, 2, 35, 36, 37, 38, 3]]"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"6Hx3mcivgMyH","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731231270,"user_tz":-540,"elapsed":1,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["# 길이 맞춰주기 위해 패딩합니다.\n","def padding(data):\n","  max_len = len(max(data, key=len))\n","  print(f\"Maximum sequence length: {max_len}\")\n","\n","  for i, seq in enumerate(tqdm(data)):\n","    if len(seq) < max_len:\n","      data[i] = seq + [pad_id] * (max_len - len(seq))\n","\n","  return data, max_len"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3e8FiNvgX60","outputId":"3a2f09b5-a20d-4afc-b7b7-25df680c41df","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731232981,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum sequence length: 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 8969.85it/s]\n"]}],"source":["data, max_len = padding(data)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"hwPSIWYugaN0","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677731234698,"user_tz":-540,"elapsed":1,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"54156578-6234-4c8c-d8d5-14e799da3d53"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[5, 6, 7, 2, 8, 9, 3, 10, 11, 0],\n"," [1, 12, 13, 14, 15, 16, 4, 0, 0, 0],\n"," [17, 1, 18, 19, 20, 21, 22, 23, 4, 0],\n"," [24, 25, 26, 27, 1, 28, 29, 30, 31, 32],\n"," [33, 34, 2, 35, 36, 37, 38, 3, 0, 0]]"]},"metadata":{},"execution_count":45}],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"rwqjACx8iidc"},"source":["### Hyperparameter 세팅 및 embedding"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"p-Ngp2nWimS8","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731237809,"user_tz":-540,"elapsed":3,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["d_model = 512  # model의 hidden size\n","num_heads = 8  # head의 개수\n","\n","# d_model이 입력을 projection 시킬 임베딩 space의 차원이므로, num_heads로 나누어 떨어져야 한다."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"GJMi2Xsni5uq","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731238234,"user_tz":-540,"elapsed":4,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["embedding = nn.Embedding(vocab_size, d_model)\n","\n","# B: batch size, L: maximum sequence length\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_emb = embedding(batch)  # (B, L, d_model)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"3tLCUQwojcUb","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677731238234,"user_tz":-540,"elapsed":3,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"6a03b622-1fdf-43e0-d7e7-6d3e3ce53592"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.1649,  1.4291, -1.5957,  ..., -1.3773, -2.3836,  0.7976],\n","         [ 0.4425, -1.3559, -0.1385,  ...,  0.7305, -0.0766, -0.0829],\n","         [-1.8174, -1.4233,  0.4261,  ...,  1.3366,  0.4890, -0.3784],\n","         ...,\n","         [ 0.2151,  1.1739,  1.1136,  ...,  0.9966,  0.6873, -0.1065],\n","         [-0.4710, -0.0341, -0.4657,  ..., -0.2656,  0.3721,  1.0845],\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252]],\n","\n","        [[-0.3132, -0.6645, -0.3721,  ...,  0.3150,  0.1436,  1.4154],\n","         [-2.2975,  0.2167, -1.6276,  ..., -0.8593, -0.5642,  0.2590],\n","         [ 0.4863,  0.3360, -1.6987,  ..., -0.4397, -1.3170,  0.8126],\n","         ...,\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252],\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252],\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252]],\n","\n","        [[ 0.6086, -2.2518,  0.4145,  ...,  0.7958, -1.9388,  1.0116],\n","         [-0.3132, -0.6645, -0.3721,  ...,  0.3150,  0.1436,  1.4154],\n","         [ 0.2429, -0.6662,  0.5199,  ...,  0.3100,  1.5763, -0.5682],\n","         ...,\n","         [ 1.0560, -1.5010,  0.0813,  ...,  0.3044, -1.2055, -0.9201],\n","         [ 0.1730,  1.8216,  0.6628,  ..., -0.2705,  1.7982,  0.1761],\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252]],\n","\n","        [[-0.8051, -0.9137,  0.3032,  ..., -0.8407,  0.1928,  0.2849],\n","         [ 0.0591, -0.9913, -0.3466,  ..., -0.8324, -0.3003, -1.0091],\n","         [ 0.2205, -0.8817,  0.4706,  ...,  1.9298, -0.2591, -0.2573],\n","         ...,\n","         [ 0.1731,  0.9920,  0.2770,  ..., -0.5021,  0.0766, -0.2792],\n","         [-0.8993, -0.4624, -0.6029,  ..., -0.6145,  0.2660,  0.3969],\n","         [ 1.8617, -0.9461,  0.8048,  ...,  1.2578,  1.2131,  1.3980]],\n","\n","        [[ 1.2148, -0.0984,  1.0630,  ...,  0.2274, -0.8476, -1.0341],\n","         [ 0.4553, -0.7445, -0.2833,  ..., -0.5268, -0.9260, -0.3948],\n","         [ 0.2790, -1.2794, -0.1183,  ...,  0.9580,  0.4651,  3.1868],\n","         ...,\n","         [-0.0481,  0.3748, -0.4840,  ...,  0.6170, -0.2142, -0.3540],\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252],\n","         [-0.2671,  0.6407, -0.0417,  ..., -0.7076,  0.9632, -1.3252]]],\n","       grad_fn=<EmbeddingBackward0>)\n","torch.Size([5, 10, 512])\n"]}],"source":["print(batch_emb)\n","print(batch_emb.shape)"]},{"cell_type":"markdown","metadata":{"id":"s0Lhx892gmi3"},"source":["### Linear projection & 여러 head로 나누기"]},{"cell_type":"markdown","metadata":{"id":"urXMBRnRgqvw"},"source":["Multi-head attention 내에서 쓰이는 linear projection matrix들을 정의합니다."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"9DWKDqgCgfMk","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731241636,"user_tz":-540,"elapsed":355,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["w_q = nn.Linear(d_model, d_model)\n","w_k = nn.Linear(d_model, d_model)\n","w_v = nn.Linear(d_model, d_model)"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"tcLuhda7m-Lm","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731241637,"user_tz":-540,"elapsed":3,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["w_0 = nn.Linear(d_model, d_model)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-vSL7PwnV6k","outputId":"ce5d776b-50be-4b87-aef6-74617c15b143","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731241637,"user_tz":-540,"elapsed":3,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 10, 512])\n","torch.Size([5, 10, 512])\n","torch.Size([5, 10, 512])\n"]}],"source":["q = w_q(batch_emb)  # (B, L, d_model)\n","k = w_k(batch_emb)  # (B, L, d_model)\n","v = w_v(batch_emb)  # (B, L, d_model)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"]},{"cell_type":"markdown","metadata":{"id":"Wnvlum-LnF1T"},"source":["Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."]},{"cell_type":"markdown","metadata":{"id":"sXcYLZYvJT_1"},"source":["- 이론적으로는 multi-head attention을 수행하면 input을 각각 다른 head 개수만큼의 Wq, Wk, Wv로 linear transformation 해서 각각 여러번의 attention 수행한 후 concat 한 후 linear transformation 수행해준다\n","- 구현에서는 Wq, Wk, Wv 한 개씩\n","- 실제 `attention is all you need` 논문의 구현 예시는 Query vector 한개를 dim으로 쪼개서 진행한다"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tiOKAv9nEli","outputId":"de39f765-800a-4581-b326-e2b50f41fafc","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731241637,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 10, 8, 64])\n","torch.Size([5, 10, 8, 64])\n","torch.Size([5, 10, 8, 64])\n"]}],"source":["batch_size = q.shape[0]\n","d_k = d_model // num_heads\n","\n","# num_heads * d_k로 쪼갠다\n","q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tNb2isfn5Cx","outputId":"e87232e2-2e6e-40ed-a634-362af16bc502","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731241637,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 8, 10, 64])\n","torch.Size([5, 8, 10, 64])\n","torch.Size([5, 8, 10, 64])\n"]}],"source":["# num_heads를 밖으로 뺌으로써\n","# 각 head가 (L, d_k) 만큼의 matrix를 가지고 self-attention 수행\n","\n","q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","print(q.shape)\n","print(k.shape)\n","print(v.shape)"]},{"cell_type":"markdown","metadata":{"id":"NWrDA5_Sofad"},"source":["### Scaled dot-product self-attention 구현"]},{"cell_type":"markdown","metadata":{"id":"w52C4k3Wfl8m"},"source":["각 head에서 실행되는 self-attetion 과정입니다."]},{"cell_type":"code","execution_count":54,"metadata":{"id":"A5waKr0Hfi2K","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677731243591,"user_tz":-540,"elapsed":3,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"568e0f0d-588b-4fbd-a8ef-e23c8e3a4ec0"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[0.1203, 0.0889, 0.1398,  ..., 0.0566, 0.0950, 0.0793],\n","          [0.0807, 0.1447, 0.1238,  ..., 0.0945, 0.0765, 0.0876],\n","          [0.1660, 0.0787, 0.1068,  ..., 0.1316, 0.1139, 0.0767],\n","          ...,\n","          [0.0812, 0.1493, 0.1321,  ..., 0.0850, 0.0854, 0.0846],\n","          [0.0571, 0.1540, 0.0882,  ..., 0.1488, 0.0568, 0.0881],\n","          [0.1155, 0.0513, 0.1685,  ..., 0.0382, 0.0619, 0.1479]],\n","\n","         [[0.0883, 0.0609, 0.1218,  ..., 0.0887, 0.1091, 0.1019],\n","          [0.1105, 0.0967, 0.1160,  ..., 0.0582, 0.0686, 0.1360],\n","          [0.0976, 0.1040, 0.1511,  ..., 0.0767, 0.0934, 0.1336],\n","          ...,\n","          [0.1507, 0.0774, 0.1474,  ..., 0.0640, 0.1208, 0.1014],\n","          [0.0889, 0.0475, 0.0583,  ..., 0.1148, 0.0949, 0.1348],\n","          [0.0712, 0.0644, 0.0755,  ..., 0.0549, 0.0594, 0.2382]],\n","\n","         [[0.1284, 0.0916, 0.0823,  ..., 0.0967, 0.1013, 0.0857],\n","          [0.0801, 0.0682, 0.2024,  ..., 0.0753, 0.0636, 0.0987],\n","          [0.0571, 0.0754, 0.1120,  ..., 0.1355, 0.0900, 0.0557],\n","          ...,\n","          [0.0665, 0.1087, 0.1064,  ..., 0.0755, 0.0701, 0.1158],\n","          [0.1277, 0.1683, 0.0733,  ..., 0.1014, 0.0574, 0.0989],\n","          [0.0850, 0.1168, 0.0689,  ..., 0.0837, 0.0913, 0.0740]],\n","\n","         ...,\n","\n","         [[0.0535, 0.1880, 0.1153,  ..., 0.1589, 0.0853, 0.0512],\n","          [0.0678, 0.0817, 0.1034,  ..., 0.0880, 0.1275, 0.0836],\n","          [0.0742, 0.1044, 0.0917,  ..., 0.1042, 0.0334, 0.1079],\n","          ...,\n","          [0.0889, 0.0750, 0.0592,  ..., 0.1996, 0.0924, 0.1204],\n","          [0.0624, 0.1227, 0.1446,  ..., 0.0826, 0.0683, 0.0882],\n","          [0.0587, 0.0881, 0.0846,  ..., 0.0900, 0.0799, 0.1188]],\n","\n","         [[0.1057, 0.1110, 0.1101,  ..., 0.1079, 0.1258, 0.0583],\n","          [0.0967, 0.0866, 0.1576,  ..., 0.0706, 0.1021, 0.1010],\n","          [0.1560, 0.0719, 0.1316,  ..., 0.1237, 0.0741, 0.0604],\n","          ...,\n","          [0.0717, 0.1274, 0.1360,  ..., 0.1371, 0.0931, 0.0472],\n","          [0.0986, 0.0927, 0.0917,  ..., 0.0949, 0.1134, 0.0503],\n","          [0.1648, 0.0821, 0.1342,  ..., 0.0936, 0.0824, 0.0836]],\n","\n","         [[0.1020, 0.0941, 0.0653,  ..., 0.0960, 0.1047, 0.0800],\n","          [0.1388, 0.0502, 0.1009,  ..., 0.1935, 0.0731, 0.1250],\n","          [0.1019, 0.0924, 0.1268,  ..., 0.1015, 0.1602, 0.0966],\n","          ...,\n","          [0.1249, 0.0619, 0.0703,  ..., 0.0764, 0.1163, 0.1337],\n","          [0.0708, 0.0951, 0.1160,  ..., 0.1128, 0.0899, 0.0955],\n","          [0.1478, 0.0544, 0.0431,  ..., 0.0481, 0.1531, 0.1264]]],\n","\n","\n","        [[[0.0735, 0.0965, 0.0946,  ..., 0.1183, 0.1183, 0.1183],\n","          [0.1174, 0.1340, 0.0668,  ..., 0.0899, 0.0899, 0.0899],\n","          [0.1317, 0.1559, 0.0773,  ..., 0.0893, 0.0893, 0.0893],\n","          ...,\n","          [0.1024, 0.0528, 0.0801,  ..., 0.1249, 0.1249, 0.1249],\n","          [0.1024, 0.0528, 0.0801,  ..., 0.1249, 0.1249, 0.1249],\n","          [0.1024, 0.0528, 0.0801,  ..., 0.1249, 0.1249, 0.1249]],\n","\n","         [[0.1563, 0.1493, 0.0828,  ..., 0.0614, 0.0614, 0.0614],\n","          [0.1263, 0.1437, 0.1540,  ..., 0.0558, 0.0558, 0.0558],\n","          [0.1593, 0.1118, 0.0494,  ..., 0.0661, 0.0661, 0.0661],\n","          ...,\n","          [0.0930, 0.0303, 0.0352,  ..., 0.1845, 0.1845, 0.1845],\n","          [0.0930, 0.0303, 0.0352,  ..., 0.1845, 0.1845, 0.1845],\n","          [0.0930, 0.0303, 0.0352,  ..., 0.1845, 0.1845, 0.1845]],\n","\n","         [[0.0739, 0.0459, 0.1044,  ..., 0.1160, 0.1160, 0.1160],\n","          [0.1385, 0.0799, 0.1076,  ..., 0.0864, 0.0864, 0.0864],\n","          [0.0625, 0.1104, 0.1307,  ..., 0.0982, 0.0982, 0.0982],\n","          ...,\n","          [0.1515, 0.0912, 0.1083,  ..., 0.0777, 0.0777, 0.0777],\n","          [0.1515, 0.0912, 0.1083,  ..., 0.0777, 0.0777, 0.0777],\n","          [0.1515, 0.0912, 0.1083,  ..., 0.0777, 0.0777, 0.0777]],\n","\n","         ...,\n","\n","         [[0.0769, 0.0833, 0.0842,  ..., 0.1227, 0.1227, 0.1227],\n","          [0.0839, 0.0946, 0.0471,  ..., 0.1203, 0.1203, 0.1203],\n","          [0.1057, 0.1265, 0.1737,  ..., 0.0622, 0.0622, 0.0622],\n","          ...,\n","          [0.0605, 0.0870, 0.0957,  ..., 0.1299, 0.1299, 0.1299],\n","          [0.0605, 0.0870, 0.0957,  ..., 0.1299, 0.1299, 0.1299],\n","          [0.0605, 0.0870, 0.0957,  ..., 0.1299, 0.1299, 0.1299]],\n","\n","         [[0.1050, 0.0898, 0.1172,  ..., 0.1022, 0.1022, 0.1022],\n","          [0.0605, 0.0804, 0.0750,  ..., 0.1286, 0.1286, 0.1286],\n","          [0.0770, 0.0686, 0.1231,  ..., 0.1333, 0.1333, 0.1333],\n","          ...,\n","          [0.1251, 0.0714, 0.0926,  ..., 0.0890, 0.0890, 0.0890],\n","          [0.1251, 0.0714, 0.0926,  ..., 0.0890, 0.0890, 0.0890],\n","          [0.1251, 0.0714, 0.0926,  ..., 0.0890, 0.0890, 0.0890]],\n","\n","         [[0.0866, 0.0869, 0.1010,  ..., 0.1251, 0.1251, 0.1251],\n","          [0.1021, 0.1268, 0.0913,  ..., 0.0991, 0.0991, 0.0991],\n","          [0.1767, 0.0921, 0.1006,  ..., 0.0763, 0.0763, 0.0763],\n","          ...,\n","          [0.1068, 0.1044, 0.1628,  ..., 0.1039, 0.1039, 0.1039],\n","          [0.1068, 0.1044, 0.1628,  ..., 0.1039, 0.1039, 0.1039],\n","          [0.1068, 0.1044, 0.1628,  ..., 0.1039, 0.1039, 0.1039]]],\n","\n","\n","        [[[0.1295, 0.0497, 0.1620,  ..., 0.1432, 0.0823, 0.0724],\n","          [0.1121, 0.0813, 0.0999,  ..., 0.0810, 0.0824, 0.1308],\n","          [0.0696, 0.0812, 0.1032,  ..., 0.0890, 0.0626, 0.1342],\n","          ...,\n","          [0.1184, 0.0631, 0.1321,  ..., 0.0911, 0.1352, 0.0749],\n","          [0.1254, 0.1214, 0.0780,  ..., 0.0997, 0.1016, 0.1404],\n","          [0.0945, 0.0967, 0.1777,  ..., 0.0545, 0.0890, 0.1180]],\n","\n","         [[0.0544, 0.1471, 0.1382,  ..., 0.0845, 0.0966, 0.0878],\n","          [0.1542, 0.1534, 0.0633,  ..., 0.0910, 0.0742, 0.0603],\n","          [0.1275, 0.1395, 0.0600,  ..., 0.0819, 0.1153, 0.0786],\n","          ...,\n","          [0.0808, 0.1043, 0.0846,  ..., 0.1063, 0.0874, 0.0835],\n","          [0.1014, 0.0775, 0.1004,  ..., 0.0844, 0.0805, 0.1305],\n","          [0.0738, 0.1069, 0.1021,  ..., 0.1066, 0.0815, 0.2122]],\n","\n","         [[0.0549, 0.1494, 0.0915,  ..., 0.0857, 0.0776, 0.0798],\n","          [0.1272, 0.0645, 0.0719,  ..., 0.1346, 0.1251, 0.1013],\n","          [0.0974, 0.1498, 0.1642,  ..., 0.0873, 0.0704, 0.0813],\n","          ...,\n","          [0.1203, 0.0827, 0.0767,  ..., 0.1220, 0.0967, 0.0799],\n","          [0.0752, 0.1619, 0.1892,  ..., 0.0692, 0.0581, 0.0832],\n","          [0.1560, 0.1107, 0.1258,  ..., 0.1524, 0.0677, 0.0568]],\n","\n","         ...,\n","\n","         [[0.0897, 0.0931, 0.0907,  ..., 0.0980, 0.0421, 0.0476],\n","          [0.0833, 0.0627, 0.0826,  ..., 0.0781, 0.0708, 0.1001],\n","          [0.0843, 0.2050, 0.1725,  ..., 0.0540, 0.1013, 0.0623],\n","          ...,\n","          [0.1434, 0.1038, 0.1341,  ..., 0.1192, 0.0586, 0.0791],\n","          [0.0804, 0.1458, 0.1175,  ..., 0.1139, 0.0960, 0.0683],\n","          [0.0920, 0.0701, 0.0676,  ..., 0.0899, 0.1346, 0.1505]],\n","\n","         [[0.1062, 0.1530, 0.0883,  ..., 0.0664, 0.1324, 0.1333],\n","          [0.1114, 0.1214, 0.1200,  ..., 0.1045, 0.0856, 0.1182],\n","          [0.0775, 0.1231, 0.1004,  ..., 0.1105, 0.0777, 0.1290],\n","          ...,\n","          [0.0684, 0.0874, 0.0425,  ..., 0.1002, 0.0943, 0.1216],\n","          [0.0903, 0.0815, 0.1321,  ..., 0.1153, 0.1110, 0.0880],\n","          [0.0741, 0.1050, 0.0760,  ..., 0.1003, 0.0833, 0.0747]],\n","\n","         [[0.0789, 0.1538, 0.0495,  ..., 0.1194, 0.0539, 0.1193],\n","          [0.0761, 0.1215, 0.1408,  ..., 0.1196, 0.0679, 0.1755],\n","          [0.0900, 0.1693, 0.0909,  ..., 0.0869, 0.0795, 0.1586],\n","          ...,\n","          [0.0704, 0.2705, 0.0918,  ..., 0.0847, 0.0756, 0.0543],\n","          [0.0897, 0.0467, 0.0432,  ..., 0.0837, 0.1115, 0.1919],\n","          [0.0855, 0.1231, 0.1226,  ..., 0.1164, 0.1198, 0.1198]]],\n","\n","\n","        [[[0.0844, 0.1760, 0.0927,  ..., 0.0570, 0.0854, 0.1232],\n","          [0.1139, 0.1007, 0.0718,  ..., 0.2052, 0.0717, 0.0785],\n","          [0.1510, 0.0527, 0.0980,  ..., 0.1174, 0.1087, 0.0885],\n","          ...,\n","          [0.1031, 0.1063, 0.0771,  ..., 0.0932, 0.0720, 0.1723],\n","          [0.0593, 0.1098, 0.0623,  ..., 0.1351, 0.1000, 0.1439],\n","          [0.1364, 0.0725, 0.0553,  ..., 0.1204, 0.1232, 0.0755]],\n","\n","         [[0.1053, 0.0874, 0.1199,  ..., 0.0759, 0.0991, 0.0728],\n","          [0.1203, 0.0855, 0.0770,  ..., 0.1000, 0.0706, 0.0967],\n","          [0.0986, 0.0759, 0.1020,  ..., 0.0539, 0.0971, 0.0887],\n","          ...,\n","          [0.1001, 0.1085, 0.0931,  ..., 0.0972, 0.1113, 0.0702],\n","          [0.0947, 0.1088, 0.1280,  ..., 0.0632, 0.0773, 0.1449],\n","          [0.1761, 0.0847, 0.0544,  ..., 0.0869, 0.0956, 0.1046]],\n","\n","         [[0.0844, 0.1437, 0.0636,  ..., 0.0582, 0.0894, 0.1312],\n","          [0.1074, 0.0915, 0.1171,  ..., 0.0918, 0.1144, 0.1039],\n","          [0.1147, 0.1047, 0.0953,  ..., 0.0899, 0.1053, 0.0916],\n","          ...,\n","          [0.0654, 0.0883, 0.0907,  ..., 0.0514, 0.1546, 0.1323],\n","          [0.0893, 0.1059, 0.1001,  ..., 0.1314, 0.1677, 0.0934],\n","          [0.0972, 0.0941, 0.1046,  ..., 0.1032, 0.0880, 0.1265]],\n","\n","         ...,\n","\n","         [[0.0643, 0.0943, 0.0971,  ..., 0.0603, 0.1168, 0.1060],\n","          [0.1034, 0.0635, 0.0862,  ..., 0.1322, 0.0710, 0.1136],\n","          [0.0895, 0.1457, 0.1198,  ..., 0.0471, 0.0676, 0.0959],\n","          ...,\n","          [0.1461, 0.0714, 0.0771,  ..., 0.1143, 0.0810, 0.0621],\n","          [0.0801, 0.1554, 0.0888,  ..., 0.0573, 0.0675, 0.0838],\n","          [0.0752, 0.1359, 0.1087,  ..., 0.1264, 0.0910, 0.1427]],\n","\n","         [[0.0823, 0.0563, 0.0641,  ..., 0.0883, 0.1386, 0.1784],\n","          [0.0751, 0.0626, 0.1077,  ..., 0.1126, 0.1135, 0.1117],\n","          [0.0863, 0.1108, 0.1431,  ..., 0.0978, 0.1106, 0.0853],\n","          ...,\n","          [0.0610, 0.0977, 0.0710,  ..., 0.0805, 0.0862, 0.0999],\n","          [0.1247, 0.1258, 0.0931,  ..., 0.0818, 0.1088, 0.0704],\n","          [0.0885, 0.0860, 0.0843,  ..., 0.0962, 0.1203, 0.1006]],\n","\n","         [[0.1017, 0.0749, 0.1297,  ..., 0.0644, 0.0960, 0.1300],\n","          [0.0587, 0.1184, 0.1225,  ..., 0.0970, 0.1062, 0.1098],\n","          [0.1206, 0.0788, 0.0703,  ..., 0.1438, 0.1285, 0.0682],\n","          ...,\n","          [0.1004, 0.1239, 0.1003,  ..., 0.1209, 0.0933, 0.1095],\n","          [0.0362, 0.1188, 0.1022,  ..., 0.0540, 0.1455, 0.0962],\n","          [0.1756, 0.0862, 0.1663,  ..., 0.0515, 0.0532, 0.0994]]],\n","\n","\n","        [[[0.1392, 0.0543, 0.1205,  ..., 0.1042, 0.0905, 0.0905],\n","          [0.1074, 0.0593, 0.0781,  ..., 0.1085, 0.1619, 0.1619],\n","          [0.1044, 0.0697, 0.1278,  ..., 0.1024, 0.0866, 0.0866],\n","          ...,\n","          [0.0460, 0.1095, 0.0844,  ..., 0.1314, 0.0923, 0.0923],\n","          [0.1033, 0.0903, 0.0762,  ..., 0.0767, 0.1126, 0.1126],\n","          [0.1033, 0.0903, 0.0762,  ..., 0.0767, 0.1126, 0.1126]],\n","\n","         [[0.0628, 0.0949, 0.1746,  ..., 0.0547, 0.0808, 0.0808],\n","          [0.0896, 0.0721, 0.0937,  ..., 0.0903, 0.0777, 0.0777],\n","          [0.1146, 0.0926, 0.0919,  ..., 0.1033, 0.0706, 0.0706],\n","          ...,\n","          [0.1111, 0.1118, 0.1099,  ..., 0.0841, 0.0870, 0.0870],\n","          [0.0791, 0.0663, 0.1086,  ..., 0.0882, 0.1875, 0.1875],\n","          [0.0791, 0.0663, 0.1086,  ..., 0.0882, 0.1875, 0.1875]],\n","\n","         [[0.1358, 0.0874, 0.0985,  ..., 0.1077, 0.0932, 0.0932],\n","          [0.0983, 0.0945, 0.1157,  ..., 0.1079, 0.0683, 0.0683],\n","          [0.1037, 0.0973, 0.0597,  ..., 0.1102, 0.0881, 0.0881],\n","          ...,\n","          [0.1132, 0.1326, 0.0691,  ..., 0.0683, 0.1231, 0.1231],\n","          [0.0875, 0.1428, 0.1870,  ..., 0.0924, 0.0834, 0.0834],\n","          [0.0875, 0.1428, 0.1870,  ..., 0.0924, 0.0834, 0.0834]],\n","\n","         ...,\n","\n","         [[0.1210, 0.1947, 0.0725,  ..., 0.1036, 0.0833, 0.0833],\n","          [0.0566, 0.1461, 0.0559,  ..., 0.1246, 0.0782, 0.0782],\n","          [0.1279, 0.0997, 0.0913,  ..., 0.1096, 0.0907, 0.0907],\n","          ...,\n","          [0.1036, 0.1929, 0.0858,  ..., 0.0702, 0.0569, 0.0569],\n","          [0.1453, 0.0647, 0.1437,  ..., 0.1188, 0.1127, 0.1127],\n","          [0.1453, 0.0647, 0.1437,  ..., 0.1188, 0.1127, 0.1127]],\n","\n","         [[0.0795, 0.0699, 0.1337,  ..., 0.0784, 0.0892, 0.0892],\n","          [0.1041, 0.0918, 0.1230,  ..., 0.0892, 0.0524, 0.0524],\n","          [0.0917, 0.1003, 0.1601,  ..., 0.1051, 0.0646, 0.0646],\n","          ...,\n","          [0.1117, 0.1216, 0.1192,  ..., 0.1273, 0.1011, 0.1011],\n","          [0.1922, 0.0734, 0.1211,  ..., 0.0692, 0.0725, 0.0725],\n","          [0.1922, 0.0734, 0.1211,  ..., 0.0692, 0.0725, 0.0725]],\n","\n","         [[0.0796, 0.1649, 0.0990,  ..., 0.0748, 0.0959, 0.0959],\n","          [0.1164, 0.0752, 0.0928,  ..., 0.1023, 0.0824, 0.0824],\n","          [0.1032, 0.1173, 0.0371,  ..., 0.1022, 0.1239, 0.1239],\n","          ...,\n","          [0.0968, 0.1265, 0.0802,  ..., 0.0788, 0.1026, 0.1026],\n","          [0.0802, 0.0794, 0.1022,  ..., 0.0966, 0.1136, 0.1136],\n","          [0.0802, 0.0794, 0.1022,  ..., 0.0966, 0.1136, 0.1136]]]],\n","       grad_fn=<SoftmaxBackward0>)\n","torch.Size([5, 8, 10, 10])\n"]}],"source":["# shape - (L, L)\n","# 같은 sequence 내에 서로 다른 token들에게 얼마나 가중치를 두고 attention을 해야하는가\n","attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","# softmax - row-wise이기 때문에 dim은 -1\n","attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","print(attn_dists)\n","print(attn_dists.shape)"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7megouWpgCck","outputId":"dd551621-9b77-4af2-cd35-cd1f961964c5","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731243591,"user_tz":-540,"elapsed":2,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 8, 10, 64])\n"]}],"source":["attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","print(attn_values.shape)"]},{"cell_type":"markdown","metadata":{"id":"LmSTaymdg-P_"},"source":["### 각 head의 결과물 병합"]},{"cell_type":"markdown","metadata":{"id":"YSdQZCk0hCNd"},"source":["각 head의 결과물을 concat하고 동일 차원으로 linear projection합니다."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaK0bpMGhQZ2","outputId":"230ac690-f21e-4441-de92-697c574e3c80","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677731246051,"user_tz":-540,"elapsed":355,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 10, 512])\n"]}],"source":["attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n","attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n","\n","print(attn_values.shape)"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"LTng_2SXhdH1","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677731246051,"user_tz":-540,"elapsed":1,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"3d53cdb2-5793-4804-fedc-8e551866ca87"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.0994, -0.1570, -0.1709,  ...,  0.0245, -0.0737, -0.1185],\n","         [ 0.0224, -0.1881, -0.1721,  ...,  0.0708,  0.0220, -0.1557],\n","         [-0.0166, -0.1710, -0.1809,  ...,  0.0805, -0.0464, -0.1088],\n","         ...,\n","         [ 0.0498, -0.1743, -0.1456,  ...,  0.0802, -0.0274, -0.1016],\n","         [ 0.0944, -0.1896, -0.2113,  ...,  0.0815, -0.0234, -0.1454],\n","         [-0.0029, -0.1904, -0.1720,  ...,  0.0619, -0.0521, -0.0912]],\n","\n","        [[ 0.0114, -0.0481, -0.2094,  ...,  0.0385,  0.0205,  0.1621],\n","         [ 0.0091, -0.0389, -0.1172,  ...,  0.0161, -0.0126,  0.1375],\n","         [ 0.0087,  0.0373, -0.1128,  ..., -0.0181,  0.0266,  0.1884],\n","         ...,\n","         [ 0.1558,  0.0251, -0.1835,  ...,  0.1119, -0.0358,  0.1411],\n","         [ 0.1558,  0.0251, -0.1835,  ...,  0.1119, -0.0358,  0.1411],\n","         [ 0.1558,  0.0251, -0.1835,  ...,  0.1119, -0.0358,  0.1411]],\n","\n","        [[-0.1066, -0.0521,  0.0267,  ...,  0.2098,  0.0577,  0.2374],\n","         [-0.1114, -0.0409,  0.0949,  ...,  0.2275,  0.0440,  0.2379],\n","         [-0.1124, -0.0325,  0.0536,  ...,  0.1842,  0.0432,  0.2450],\n","         ...,\n","         [-0.1085,  0.0120,  0.0154,  ...,  0.2264,  0.0554,  0.2986],\n","         [-0.1524, -0.0880,  0.0517,  ...,  0.2095, -0.0108,  0.2298],\n","         [-0.0276, -0.0550,  0.0389,  ...,  0.2146, -0.0029,  0.2025]],\n","\n","        [[-0.1527,  0.1867,  0.1433,  ...,  0.1071, -0.0562,  0.0459],\n","         [-0.1475,  0.1561,  0.0812,  ...,  0.0746, -0.0563,  0.0571],\n","         [-0.1270,  0.0923,  0.1083,  ...,  0.0472, -0.0956,  0.0523],\n","         ...,\n","         [-0.1288,  0.1096,  0.0980,  ...,  0.0702, -0.0172,  0.0911],\n","         [-0.1780,  0.1520,  0.1126,  ...,  0.1724, -0.0712,  0.0259],\n","         [-0.1209,  0.1407,  0.1847,  ...,  0.0654, -0.0356,  0.0297]],\n","\n","        [[ 0.1015, -0.1718, -0.1766,  ...,  0.1576,  0.0276,  0.0141],\n","         [ 0.1219, -0.1288, -0.1798,  ...,  0.1476,  0.0627,  0.0448],\n","         [ 0.0755, -0.1656, -0.2133,  ...,  0.1476,  0.0459,  0.0316],\n","         ...,\n","         [ 0.1021, -0.1543, -0.0619,  ...,  0.1477,  0.0408,  0.0619],\n","         [ 0.1072, -0.2010, -0.1784,  ...,  0.2313,  0.0017,  0.0385],\n","         [ 0.1072, -0.2010, -0.1784,  ...,  0.2313,  0.0017,  0.0385]]],\n","       grad_fn=<ViewBackward0>)\n","torch.Size([5, 10, 512])\n"]}],"source":["# w_0 : (d_model, d_model)\n","# 서로 다른 의미로 foucsing 된 각 head의 self-attention 정보들을 합쳐주는 역할 수행\n","outputs = w_0(attn_values)\n","\n","print(outputs)\n","print(outputs.shape)"]},{"cell_type":"markdown","metadata":{"id":"goX70VKqhxQH"},"source":["## Req. 1-3 Multi-head self-attention 모듈 클래스 구현"]},{"cell_type":"markdown","metadata":{"id":"WtNyV7mMj7V_"},"source":["위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈 class를 구현하겠습니다.\n","\n","아래 코드의 TODO 부분을 채워주세요."]},{"cell_type":"code","execution_count":67,"metadata":{"id":"U_kNhOTrkBHm","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677740191098,"user_tz":-540,"elapsed":878,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["class MultiheadAttention(nn.Module):\n","  def __init__(self):\n","    super(MultiheadAttention, self).__init__()\n","\n","    # Q, K, V learnable matrices\n","    self.w_q = nn.Linear(d_model, d_model)\n","    self.w_k = nn.Linear(d_model, d_model)\n","    self.w_v = nn.Linear(d_model, d_model)\n","\n","    # Linear projection for concatenated outputs\n","    self.w_0 = nn.Linear(d_model, d_model)\n","\n","  # scaled-dot product attention\n","  def self_attention(self, q, k, v):\n","    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n","    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n","\n","    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n","\n","    return attn_values\n","\n","  def forward(self, q, k, v):\n","    batch_size = q.shape[0]\n","\n","    # linear projection\n","    ################################################################################\n","    # TODO 1: Implement the forward pass for linear projection.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    q = self.w_q(q) # (B, L, d_model)\n","    k = self.w_k(k) # (B, L, d_model)\n","    v = self.w_v(v) # (B, L, d_model)\n","    \n","\n","    # head만큼 쪼개준다\n","    ################################################################################\n","    # TODO 2: Implement the forward pass for \bsplit head.                #\n","    ################################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # split heads\n","    q = q.view(batch_size, -1, num_heads, d_k) # (B, num_heads, L, d_k)\n","    k = k.view(batch_size, -1, num_heads, d_k) # (B, num_heads, L, d_k)\n","    v = v.view(batch_size, -1, num_heads, d_k) # (B, num_heads, L, d_k)\n","\n","\n","    # 각 head가 (L, d_k)의 matrix를 담당하도록 만든다\n","    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n","    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n","\n","    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n","    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\n","\n","    return self.w_0(attn_values)"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"jYLuu_9alQxT","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677740194012,"user_tz":-540,"elapsed":491,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":["multihead_attn = MultiheadAttention()\n","\n","outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"KMiXlYjSlTfB","vscode":{"languageId":"python"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677740195919,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤성운","userId":"06802376574597247220"}},"outputId":"a9c3d81c-f932-463f-8237-c14d92939649"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-5.7519e-02,  8.3672e-02,  3.5657e-03,  ..., -1.2171e-01,\n","           1.0617e-01,  1.3804e-02],\n","         [ 1.9101e-02,  1.9733e-02,  3.3341e-02,  ..., -1.3445e-01,\n","           3.9254e-02,  1.3933e-02],\n","         [-1.0876e-01,  1.6079e-02, -3.1137e-02,  ..., -2.2008e-01,\n","           8.2375e-02,  4.9501e-02],\n","         ...,\n","         [ 1.1818e-02,  9.5664e-02, -2.9019e-02,  ..., -3.9713e-02,\n","           5.2620e-02,  4.7286e-02],\n","         [ 8.0589e-03,  1.0880e-01, -3.2934e-02,  ..., -1.6008e-01,\n","           1.5856e-01,  8.1992e-02],\n","         [-1.5246e-02,  7.1493e-02, -6.8657e-02,  ..., -1.1417e-01,\n","           5.4012e-02,  3.8931e-02]],\n","\n","        [[ 1.1694e-01,  7.3469e-02, -1.2465e-01,  ..., -4.6682e-02,\n","          -1.0961e-01, -2.2308e-01],\n","         [ 1.3113e-01,  5.8297e-02, -1.5927e-01,  ..., -3.3941e-02,\n","          -1.6876e-01, -1.8437e-01],\n","         [ 1.6579e-01,  6.9502e-02, -8.0857e-02,  ..., -9.2919e-02,\n","          -9.6314e-02, -2.8773e-01],\n","         ...,\n","         [ 1.8572e-01,  3.7856e-02, -1.0394e-01,  ...,  3.1802e-02,\n","          -1.6707e-01, -1.8687e-01],\n","         [ 1.8572e-01,  3.7856e-02, -1.0394e-01,  ...,  3.1802e-02,\n","          -1.6707e-01, -1.8687e-01],\n","         [ 1.8572e-01,  3.7856e-02, -1.0394e-01,  ...,  3.1802e-02,\n","          -1.6707e-01, -1.8687e-01]],\n","\n","        [[-6.2597e-02,  9.6737e-02,  7.3341e-02,  ..., -7.6904e-02,\n","          -1.1137e-01, -7.1057e-02],\n","         [-2.3817e-02,  4.9314e-02,  7.3142e-02,  ..., -6.8203e-02,\n","          -7.3158e-02, -7.7546e-02],\n","         [-8.6379e-03,  4.3591e-02,  5.4068e-02,  ...,  2.1290e-04,\n","          -9.1399e-02, -1.2114e-01],\n","         ...,\n","         [-3.7305e-02,  1.4272e-01,  3.6082e-02,  ..., -6.6453e-02,\n","          -6.4983e-02, -5.7242e-02],\n","         [ 1.2653e-02,  1.0387e-01,  9.2628e-03,  ..., -2.7666e-02,\n","          -5.4358e-02, -7.7521e-02],\n","         [ 4.2499e-03,  2.3412e-02,  9.1966e-02,  ...,  3.8873e-02,\n","          -7.0921e-02, -8.2464e-02]],\n","\n","        [[-1.0990e-01, -9.6826e-02,  6.5159e-02,  ...,  2.6238e-03,\n","          -3.1068e-02, -1.4631e-01],\n","         [-1.3607e-01, -6.5648e-02,  4.1053e-02,  ..., -3.1946e-03,\n","          -1.0285e-01, -1.8521e-01],\n","         [-1.4493e-01, -1.3097e-01,  3.5026e-02,  ..., -3.0943e-03,\n","          -4.6571e-02, -2.0411e-01],\n","         ...,\n","         [-1.9105e-01, -9.3264e-02,  1.0014e-02,  ..., -3.1407e-02,\n","          -1.3452e-01, -1.4864e-01],\n","         [-1.5881e-01, -4.9801e-02,  1.1706e-03,  ..., -1.4205e-02,\n","          -9.6098e-02, -1.5964e-01],\n","         [-1.3879e-01, -8.0869e-02,  1.2789e-02,  ...,  2.6500e-02,\n","          -9.0259e-02, -1.8196e-01]],\n","\n","        [[ 7.0132e-02,  2.8322e-02, -1.0587e-02,  ..., -1.5885e-03,\n","          -4.6914e-03,  1.3968e-03],\n","         [ 1.1262e-02,  8.7033e-02,  9.1836e-05,  ..., -1.8213e-03,\n","           1.0509e-02, -1.4915e-02],\n","         [ 1.3498e-01,  1.8064e-02, -2.0493e-02,  ..., -2.4185e-02,\n","           1.9139e-02, -1.4648e-02],\n","         ...,\n","         [ 5.8513e-02, -1.4349e-02, -5.5347e-02,  ..., -7.9259e-05,\n","           1.9646e-02,  5.2369e-02],\n","         [ 4.8750e-02, -9.4102e-03, -3.2702e-02,  ...,  6.5561e-02,\n","          -4.2108e-02,  9.7826e-03],\n","         [ 4.8750e-02, -9.4102e-03, -3.2702e-02,  ...,  6.5561e-02,\n","          -4.2108e-02,  9.7826e-03]]], grad_fn=<ViewBackward0>)\n","torch.Size([5, 10, 512])\n"]}],"source":["print(outputs)\n","print(outputs.shape)  # (batch_size, length, d_model)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"OTku1fySVR3L","vscode":{"languageId":"python"},"executionInfo":{"status":"ok","timestamp":1677729359510,"user_tz":-540,"elapsed":1,"user":{"displayName":"윤성운","userId":"06802376574597247220"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["8qRU5DFY2OM8","NWrDA5_Sofad"],"provenance":[{"file_id":"1XNOcAyN3Q9KN6-9oA1t6Ueh37tJYQFGa","timestamp":1658069359050}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}